<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>) &mdash; PLite.jl 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="PLite.jl 1.0 documentation" href="#" />
   
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9">

  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="#">PLite.jl 1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <p>&lt;!&#8211; START doctoc generated TOC please keep comment here to allow auto update &#8211;&gt;
&lt;!&#8211; DON&#8217;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE &#8211;&gt;
<strong>Table of Contents</strong>  <em>generated with [DocToc](https://github.com/thlorenz/doctoc)</em></p>
<ul>
<li><p class="first">[PLite.jl](#plitejl)
- [Problem definition](#problem-definition)</p>
<blockquote>
<div><ul>
<li><p class="first">[MDP](#mdp)
- [State space](#state-space)
- [Action space](#action-space)
- [Transition](#transition)</p>
<blockquote>
<div><ul class="simple">
<li>[<em>T*(*s</em>, <em>a</em>, <em>s</em>&#8216;) type transition](#ts-a-s-type-transition)</li>
<li>[<em>T*(*s</em>, <em>a</em>) type transition](#ts-a-type-transition)</li>
</ul>
</div></blockquote>
<ul class="simple">
<li>[Reward](#reward)</li>
</ul>
</li>
</ul>
</div></blockquote>
<ul>
<li><p class="first">[Solver selection](#solver-selection)
- [Serial Value iteration](#serial-value-iteration)</p>
<blockquote>
<div><ul class="simple">
<li>[Infinite horizon with discount](#infinite-horizon-with-discount)</li>
<li>[Finite horizon without discount](#finite-horizon-without-discount)</li>
<li>[Example](#example)
- [MDP with <em>T*(*s</em>, <em>a</em>, <em>s</em>&#8216;) type transition](#mdp-with-ts-a-s-type-transition)
- [MDP with <em>T*(*s</em>, <em>a</em>) type transition](#mdp-with-ts-a-type-transition)</li>
</ul>
</div></blockquote>
<ul class="simple">
<li>[Parallel Value Iteration](#parallel-value-iteration)</li>
<li>[Monte-Carlo Tree Search](#monte-carlo-tree-search)</li>
</ul>
</li>
<li><p class="first">[Solution](#solution)</p>
</li>
</ul>
</li>
<li><p class="first">[Todos](#todos)
- [Short-term](#short-term)
- [Medium-term](#medium-term)</p>
</li>
<li><p class="first">[Footnotes](#footnotes)</p>
</li>
</ul>
<p>&lt;!&#8211; END doctoc generated TOC please keep comment here to allow auto update &#8211;&gt;</p>
<p># PLite.jl</p>
<p>PLite, pronounced &#8220;polite,&#8221;&lt;sup&gt;[1](#myfootnote1)&lt;/sup&gt; is a Julia-based modeling system for Markov decision processes. PLite turns Julia into a modeling language, allowing the stochastic problem to be specified using simple Julia expression syntax.</p>
<p>## Problem definition</p>
<p>### MDP</p>
<p>In a Markov decision process (MDP), an agent chooses action <em>a</em> based on observing state <em>s</em>. The agent then receives a reward <em>r</em>. The state evolves stochastically based on the current state and action taken by the agent. Note that the next state depends only on the current state and action, and not on any prior state or action. This assumption is known as the Markov assumption.</p>
<p>An MDP is typically defined by the tuple (<em>S</em>, <em>A</em>, <em>T</em>, <em>R</em>), where</p>
<ul class="simple">
<li><em>S</em> is the state space</li>
<li><em>A</em> is the action space</li>
<li><em>T*(*s</em>, <em>a</em>, <em>s</em>&#8216;) is the transition function that gives the probability of reaching state <em>s</em>&#8216; from state s by taking action <em>a</em></li>
<li><em>R*(*s</em>, <em>a</em>) is the reward function that gives a scalar value for taking action <em>a</em> at state <em>s</em>.</li>
</ul>
<p>To define an MDP, simply type the following.</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">mdp</span> <span class="pre">=</span> <span class="pre">MDP()</span>
<span class="pre">`</span></code></p>
<p>#### State space</p>
<p>The state space <em>S</em> is the set of all states, and it can be continuous and thus infinite. PLite provides the ability to define states simply as a set of all states or in a factored representation.</p>
<p>To define <em>S</em> the latter way, simply define the range or the array of possible values for the state variable. For instance, to define range of values for the continuous state variable <em>x</em> from 0 to 100, we simply type the following.</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">statevariable!(mdp,</span> <span class="pre">&quot;x&quot;,</span> <span class="pre">0,</span> <span class="pre">100)</span>
<span class="pre">`</span></code></p>
<p>Note that we stringified <em>x</em> as <cite>&#8220;x&#8221;</cite>. Internally, this syntax allows <cite>mdp</cite> to keep an internal representation of the variable.</p>
<p>At this point, we may be tempted to discretize the variable and use something like value iteration to solve the MDP. The reason we don&#8217;t provide the discretization option is because discretization is an approximation technique that should be considered together with the solution method. Yes, you may decide to discretize it an input the discretized values as an array input (see below). But we emphasize that at this point, we&#8217;re only worried about defining a mathematical formulation of the problem.</p>
<p>To define a set of discrete values for a discrete state variable <em>direction</em> that can take on the values <em>north</em>, <em>south</em>, <em>east</em>, and <em>west</em>, we simply type the following.</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">statevariable!(mdp,</span> <span class="pre">&quot;direction&quot;,</span> <span class="pre">[&quot;north&quot;,</span> <span class="pre">&quot;south&quot;,</span> <span class="pre">&quot;east&quot;,</span> <span class="pre">&quot;weast&quot;])</span>
<span class="pre">`</span></code></p>
<p>If you made a mistake, as we did above with the spelling of <em>west</em>, we can override the existing definition of the state variable <em>direction</em> by redefining it as follows.</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">statevariable!(mdp,</span> <span class="pre">&quot;direction&quot;,</span> <span class="pre">[&quot;north&quot;,</span> <span class="pre">&quot;south&quot;,</span> <span class="pre">&quot;east&quot;,</span> <span class="pre">&quot;west&quot;])</span>
<span class="pre">`</span></code></p>
<p>PLite will provide a warning whenever you redefine a previously named variable.</p>
<p>To define <em>S</em> as a set of all states, we can simply define a single discrete state variable and type the following.</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">statevariable!(mdp,</span> <span class="pre">&quot;S&quot;,</span> <span class="pre">[&quot;v1&quot;,</span> <span class="pre">&quot;v2&quot;,</span> <span class="pre">&quot;v3&quot;,</span> <span class="pre">&quot;v4&quot;,</span> <span class="pre">&quot;v5&quot;,</span> <span class="pre">&quot;v6&quot;,</span> <span class="pre">&quot;v7&quot;])</span>
<span class="pre">`</span></code></p>
<p>Note that <cite>&#8220;S&#8221;</cite> is not a special keyword. It&#8217;s just a state variable like any other, and you&#8217;ll have to take care to treat it as a state variable in defining the transition and reward functions. The nice thing is PLite allows you to work with both factored and unfactored MDP state space representations.</p>
<p>#### Action space</p>
<p>The action space <em>A</em> is the set of all actions. Like <em>S</em>, it can be continuous and thus infinite. The definition of action variables follow that of state variables, which means that PLite allows both factored and unfactored action space definitions.</p>
<p>To define a continuous and a discrete action variable <em>bankangle</em> and <em>move</em>, respectively, we type</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">actionvariable!(mdp,</span> <span class="pre">&quot;bankangle&quot;,</span> <span class="pre">-180,</span> <span class="pre">180)</span>
<span class="pre">actionvariable!(mdp,</span> <span class="pre">&quot;move&quot;,</span> <span class="pre">[&quot;up&quot;,</span> <span class="pre">&quot;down&quot;,</span> <span class="pre">&quot;left&quot;,</span> <span class="pre">&quot;right&quot;])</span>
<span class="pre">`</span></code></p>
<p>#### Transition</p>
<p>PLite provides two ways to define a transition model. Depending on the problem, it may be easier to define the transition model one way or another.</p>
<p>##### <em>T*(*s</em>, <em>a</em>, <em>s</em>&#8216;) type transition</p>
<p>The first method follows the standard definition that returns a probability value between 0 and 1. Suppose we have the following definition of an MDP.</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>julia
# constants
const MinX = 0
const MaxX = 100</p>
<p># mdp definition
mdp = MDP()</p>
<p>statevariable!(mdp, &#8220;x&#8221;, MinX, MaxX)  # continuous
statevariable!(mdp, &#8220;goal&#8221;, [&#8220;yes&#8221;, &#8220;no&#8221;])  # discrete</p>
<p>actionvariable!(mdp, &#8220;move&#8221;, [&#8220;W&#8221;, &#8220;E&#8221;, &#8220;stop&#8221;])  # discrete
<a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>We want to define a transition function that takes a state-action-next state triplet (<em>s</em>, <em>a</em>, <em>s</em>&#8216;) and returns the probability of starting in state <em>s</em>, taking action <em>a</em>, and ending up in state <em>s</em>&#8216;. Internally, PLite needs to match the defined state and action variables with the corresponding arguments for the transition function. To do this, we need to pass in an array of the argument names in the order they are to be input to the defined transition function.</p>
<p>An example of a <em>T*(*s</em>, <em>a</em>, <em>s</em>&#8216;) type transition function is as follows. Here, the state variables are named <cite>&#8220;x&#8221;</cite> and <cite>&#8220;goal&#8221;</cite>, and the action variable is named <cite>&#8220;move&#8221;</cite>. Note that although <em>s</em>&#8216; is a different variable from <em>s</em>, they share the variable names <cite>&#8220;x&#8221;</cite> and <cite>&#8220;goal&#8221;</cite>. So even though the <cite>mytransition</cite> function signature is</p>
<p><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>julia
function mytransition(</p>
<blockquote>
<div>x::Float64,
goal::String,
move::String,
xp::Float64,
goalp::String)</div></blockquote>
<p><a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
<p>the array of (ordered) argument names is <cite>[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;, &#8220;x&#8221;, &#8220;goal&#8221;]</cite> rather than <cite>[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;, &#8220;xp&#8221;, &#8220;goalp&#8221;]</cite>. Below is the full listing that defines the transition for <cite>mdp</cite>.</p>
<p><a href="#id17"><span class="problematic" id="id18">``</span></a><a href="#id19"><span class="problematic" id="id20">`</span></a>julia
transition!(mdp,</p>
<blockquote>
<div><dl class="docutils">
<dt>[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;, &#8220;x&#8221;, &#8220;goal&#8221;],  # note <a href="#id92"><span class="problematic" id="id93">|xp|</span></a> is an &#8220;x&#8221; variable</dt>
<dd># note (s,a,s&#8217;) order</dd>
<dt>function mytransition(</dt>
<dd><blockquote class="first">
<div>x::Float64,
goal::String,
move::String,
xp::Float64,
goalp::String)</div></blockquote>
<dl class="docutils">
<dt>function internaltransition(x::Float64, goal::String, move::String)</dt>
<dd><dl class="first docutils">
<dt>function isgoal(x::Float64)</dt>
<dd><dl class="first docutils">
<dt>if abs(x - MaxX / 2) &lt; StepX</dt>
<dd>return &#8220;yes&#8221;</dd>
<dt>else</dt>
<dd>return &#8220;no&#8221;</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if isgoal(x) == &#8220;yes&#8221; &amp;&amp; goal == &#8220;yes&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if move == &#8220;E&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.9),
([x - StepX, isgoal(x - StepX)], 0.1)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.2),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.1),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;W&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.9)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd>return [
([x, isgoal(x)], 0.9),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.8),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;stop&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>end</p>
<p>statepprobs = internaltransition(x, goal, move)
for statepprob in statepprobs</p>
<blockquote>
<div><dl class="docutils">
<dt>if xp == statepprob[1][1] &amp;&amp; goalp == statepprob[1][2]</dt>
<dd>return statepprob[2]</dd>
</dl>
<p>end</p>
</div></blockquote>
<p class="last">end
return 0</p>
</dd>
</dl>
<p>end</p>
</div></blockquote>
<div class="section" id="id21">
<h1>)<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h1>
<p>##### <em>T*(*s</em>, <em>a</em>) type transition</p>
<p>The second way to define a transition model is to take in a state-action pair and return the set of all possible next states with their corresponding probabilities. Again, we need to pass an array of argument names in the order the (<em>s</em>, <em>a</em>) pair is defined to the transition function. Below is the full listing that defines the transition this way. It is mathematically equivalent to the <em>T*(*s</em>, <em>a</em>, <em>s</em>&#8216;) type transition defined above.</p>
<p><a href="#id22"><span class="problematic" id="id23">``</span></a><a href="#id24"><span class="problematic" id="id25">`</span></a>julia
transition!(mdp,</p>
<blockquote>
<div><p>[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;],
function mytransition(x::Float64, goal::AbstractString, move::AbstractString)</p>
<blockquote>
<div><dl class="docutils">
<dt>function isgoal(x::Float64)</dt>
<dd><dl class="first docutils">
<dt>if abs(x - MaxX / 2) &lt; StepX</dt>
<dd>return &#8220;yes&#8221;</dd>
<dt>else</dt>
<dd>return &#8220;no&#8221;</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if isgoal(x) == &#8220;yes&#8221; &amp;&amp; goal == &#8220;yes&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if move == &#8220;E&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.9),
([x - StepX, isgoal(x - StepX)], 0.1)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.2),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.1),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;W&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.9)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd>return [
([x, isgoal(x)], 0.9),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.8),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;stop&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p>end</p>
</div></blockquote>
<p>end</p>
</div></blockquote>
</div>
<div class="section" id="id26">
<h1>)<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h1>
<p>#### Reward</p>
<p>The reward function takes in a state-action pair (<em>s</em>, <em>a</em>) and returns a scalar value indicating the expected reward received when executing action <em>a</em> from state <em>s</em>. We assume that the reward function is a deterministic function of <em>s</em> and <em>a</em>.</p>
<p>The process of defining the reward function is similar to that for the <em>T*(*s</em>, <em>a</em>) type transition function. We need to pass in an ordered array of variable names for PLite&#8217;s internal housekeeping.</p>
<p><a href="#id27"><span class="problematic" id="id28">``</span></a><a href="#id29"><span class="problematic" id="id30">`</span></a>julia
reward!(mdp,</p>
<blockquote>
<div><dl class="docutils">
<dt>[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;],  # note (s,a) order</dt>
<dd># note consistency of variables order with transition</dd>
<dt>function myreward(x::Float64, goal::String, move::String)</dt>
<dd><dl class="first docutils">
<dt>if goal == &#8220;yes&#8221; &amp;&amp; move == &#8220;stop&#8221;</dt>
<dd>return 1</dd>
<dt>else</dt>
<dd>return 0</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>end</p>
</div></blockquote>
</div>
<div class="section" id="id31">
<h1>)<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h1>
<p>## Solver selection</p>
<p>PLite is intended to provide several solvers for any MDP problem, ranging from classic dynamic programming methods such as value iteration, policy iteration, and policy evaluation to approximate, online, and direct policy search methods. Eventually, we&#8217;ll include learning algorithms, including cooler ones like [deep reinforcement learning with double Q-learning](<a class="reference external" href="http://arxiv.org/abs/1509.06461">http://arxiv.org/abs/1509.06461</a>) with support for [distributed computing](<a class="reference external" href="http://arxiv.org/abs/1507.04296">http://arxiv.org/abs/1507.04296</a>).</p>
<p>Until then, we just have to play with good o&#8217; value iteration in both its serial and parallel flavors.</p>
<p>### Serial Value iteration</p>
<p>PLite implements the value iteration algorithm for infinite horizon problems with discount, but we demonstrate how to hack the existing algorithm to solve finite horizon problems with no discount.</p>
<p>#### Infinite horizon with discount</p>
<p>To initialize the serial value iteration solver, simply type the following.</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">solver</span> <span class="pre">=</span> <span class="pre">SerialValueIteration()</span>
<span class="pre">`</span></code></p>
<p>In PLite, value iteration requires all variables to be discretized. In the above problem, we need to discretize <cite>&#8220;x&#8221;</cite>, so we write</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">const</span> <span class="pre">StepX</span> <span class="pre">=</span> <span class="pre">20</span>
<span class="pre">discretize_statevariable!(solver,</span> <span class="pre">&quot;x&quot;,</span> <span class="pre">StepX)</span>
<span class="pre">`</span></code></p>
<p>Note that the solver uses the <cite>GridInterpolations.jl</cite> package for multilinear interpolation to approximate the values between the discretized state variable values if the <em>T*(*s</em>, <em>a</em>) type transition is defined. In the <em>T*(*s</em>, <em>a</em>, <em>s</em>&#8216;) type transition, PLite assumes that for any (<em>s</em>, <em>a</em>, <em>s</em>&#8216;) triplet the transition function will return a valid probability. In this case, the user is assumed to have defined a consistent MDP problem and no approximation is done on the part of PLite.</p>
<p>In any case, to solve the problem, simply pass both <cite>mdp</cite> and <cite>solver</cite> to the <cite>solve</cite> function.
<code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">solution</span> <span class="pre">=</span> <span class="pre">solve(mdp,</span> <span class="pre">solver)</span>
<span class="pre">`</span></code></p>
<p>#### Finite horizon without discount</p>
<p>Notice that both the serial and parallel value iteration solvers are built with infinite horizon problems in mind. It&#8217;s easy, however, to modify it to solve finite horizon problems by simply changing the parameters of the solvers.</p>
<p>For an MDP with a horizon of 40 and no discounting, we can define the solver as follows.</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">solver</span> <span class="pre">=</span> <span class="pre">SerialValueIteration(maxiter=40,</span> <span class="pre">discount=1)</span>
<span class="pre">`</span></code></p>
<p>There are three parameters for the <cite>SerialValueIteration</cite> solver: <cite>maxiter</cite>, <cite>tol</cite>, and <cite>discount</cite>. As their names suggest, these parameters correspond to the maximum number of iterations the value iteration algorithm will run before timing out, the L-infinity tolerance for Q-value convergence between iterations, and the discount factor, respectively.</p>
<p>The default parameters are</p>
<ul class="simple">
<li><cite>maxiter = 1000</cite></li>
<li><cite>tol = 1e-4</cite></li>
<li><cite>discount = 0.99</cite></li>
<li><cite>verbose = true</cite>.</li>
</ul>
<p>To change these parameters, we use keyword arguments when instantiating the solver object. For instance, we can define the following.</p>
<p><a href="#id32"><span class="problematic" id="id33">``</span></a><a href="#id34"><span class="problematic" id="id35">`</span></a>julia
solver = SerialValueIteration(</p>
<blockquote>
<div>tol=1e-6,
maxiter=10000,
discount=0.999,
verbose=false)</div></blockquote>
<p><a href="#id36"><span class="problematic" id="id37">``</span></a><a href="#id38"><span class="problematic" id="id39">`</span></a></p>
<p>Note that because we&#8217;re using keyword arguments (i.e., <cite>keyword=value</cite> type arguments), we can input the arguments in any way we want.</p>
<p>#### Example</p>
<p>Below are two comprehensive listings that define the simple MDP example given above, select their solvers, and extract their solutions in the form of policy functions.</p>
<p>##### MDP with <em>T*(*s</em>, <em>a</em>, <em>s</em>&#8216;) type transition</p>
<p><a href="#id40"><span class="problematic" id="id41">``</span></a><a href="#id42"><span class="problematic" id="id43">`</span></a>julia
using PLite</p>
<p># constants
const MinX = 0
const MaxX = 100
const StepX = 20</p>
<p># mdp definition
mdp = MDP()</p>
<p>statevariable!(mdp, &#8220;x&#8221;, MinX, MaxX)  # continuous
statevariable!(mdp, &#8220;goal&#8221;, [&#8220;yes&#8221;, &#8220;no&#8221;])  # discrete</p>
<p>actionvariable!(mdp, &#8220;move&#8221;, [&#8220;W&#8221;, &#8220;E&#8221;, &#8220;stop&#8221;])  # discrete</p>
<dl class="docutils">
<dt>transition!(mdp,</dt>
<dd><dl class="first docutils">
<dt>[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;, &#8220;x&#8221;, &#8220;goal&#8221;],  # note <a href="#id94"><span class="problematic" id="id95">|xp|</span></a> is an &#8220;x&#8221; variable</dt>
<dd># note (s,a,s&#8217;) order</dd>
<dt>function mytransition(</dt>
<dd><blockquote class="first">
<div>x::Float64,
goal::String,
move::String,
xp::Float64,
goalp::String)</div></blockquote>
<dl class="docutils">
<dt>function internaltransition(x::Float64, goal::String, move::String)</dt>
<dd><dl class="first docutils">
<dt>function isgoal(x::Float64)</dt>
<dd><dl class="first docutils">
<dt>if abs(x - MaxX / 2) &lt; StepX</dt>
<dd>return &#8220;yes&#8221;</dd>
<dt>else</dt>
<dd>return &#8220;no&#8221;</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if isgoal(x) == &#8220;yes&#8221; &amp;&amp; goal == &#8220;yes&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if move == &#8220;E&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.9),
([x - StepX, isgoal(x - StepX)], 0.1)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.2),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.1),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;W&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.9)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd>return [
([x, isgoal(x)], 0.9),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.8),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;stop&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>end</p>
<p>statepprobs = internaltransition(x, goal, move)
for statepprob in statepprobs</p>
<blockquote>
<div><dl class="docutils">
<dt>if xp == statepprob[1][1] &amp;&amp; goalp == statepprob[1][2]</dt>
<dd>return statepprob[2]</dd>
</dl>
<p>end</p>
</div></blockquote>
<p class="last">end
return 0</p>
</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>)</p>
<dl class="docutils">
<dt>reward!(mdp,</dt>
<dd><dl class="first docutils">
<dt>[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;],  # note (s,a) order</dt>
<dd># note consistency of variables order with transition</dd>
<dt>function myreward(x::Float64, goal::String, move::String)</dt>
<dd><dl class="first docutils">
<dt>if goal == &#8220;yes&#8221; &amp;&amp; move == &#8220;stop&#8221;</dt>
<dd>return 1</dd>
<dt>else</dt>
<dd>return 0</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>)</p>
<p># solver options
solver = SerialValueIteration()
discretize_statevariable!(solver, &#8220;x&#8221;, StepX)</p>
<p># generate results
solution = solve(mdp, solver)
<a href="#id44"><span class="problematic" id="id45">``</span></a><a href="#id46"><span class="problematic" id="id47">`</span></a></p>
<p>##### MDP with <em>T*(*s</em>, <em>a</em>) type transition</p>
<p><a href="#id48"><span class="problematic" id="id49">``</span></a><a href="#id50"><span class="problematic" id="id51">`</span></a>julia
using PLite</p>
<p># constants
const MinX = 0
const MaxX = 100
const StepX = 20</p>
<p># mdp definition
mdp = MDP()</p>
<p>statevariable!(mdp, &#8220;x&#8221;, MinX, MaxX)  # continuous
statevariable!(mdp, &#8220;goal&#8221;, [&#8220;no&#8221;, &#8220;yes&#8221;])  # discrete</p>
<p>actionvariable!(mdp, &#8220;move&#8221;, [&#8220;W&#8221;, &#8220;E&#8221;, &#8220;stop&#8221;])  # discrete</p>
<dl class="docutils">
<dt>transition!(mdp,</dt>
<dd><p class="first">[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;],
function mytransition(x::Float64, goal::AbstractString, move::AbstractString)</p>
<blockquote>
<div><dl class="docutils">
<dt>function isgoal(x::Float64)</dt>
<dd><dl class="first docutils">
<dt>if abs(x - MaxX / 2) &lt; StepX</dt>
<dd>return &#8220;yes&#8221;</dd>
<dt>else</dt>
<dd>return &#8220;no&#8221;</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if isgoal(x) == &#8220;yes&#8221; &amp;&amp; goal == &#8220;yes&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if move == &#8220;E&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.9),
([x - StepX, isgoal(x - StepX)], 0.1)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.2),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.1),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;W&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.9)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd>return [
([x, isgoal(x)], 0.9),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.8),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;stop&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p>end</p>
</div></blockquote>
<p class="last">end</p>
</dd>
</dl>
<p>)</p>
<dl class="docutils">
<dt>reward!(mdp,</dt>
<dd><p class="first">[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;],
function myreward(x::Float64, goal::String, move::String)</p>
<blockquote>
<div><dl class="docutils">
<dt>if goal == &#8220;yes&#8221; &amp;&amp; move == &#8220;stop&#8221;</dt>
<dd>return 1</dd>
<dt>else</dt>
<dd>return 0</dd>
</dl>
<p>end</p>
</div></blockquote>
<p class="last">end</p>
</dd>
</dl>
<p>)</p>
<p># solver options
solver = SerialValueIteration()
discretize_statevariable!(solver, &#8220;x&#8221;, StepX)</p>
<p># generate results
solution = solve(mdp, solver)
<a href="#id52"><span class="problematic" id="id53">``</span></a><a href="#id54"><span class="problematic" id="id55">`</span></a></p>
<p>### Parallel Value Iteration</p>
<p>To reap the benefits of Julia&#8217;s parallel computing framework for value iteration, we need a few more steps. The main issue we have to get around is code availability when we add processes. But we&#8217;ll skip an in-depth explanation&lt;sup&gt;[2](#myfootnote2)&lt;/sup&gt; and just go straight to what we can do&lt;sup&gt;[3](#myfootnote3)&lt;/sup&gt;.</p>
<p>We consider a quick and dirty example of running the exact same code as in the MDP with <em>T*(*s</em>, <em>a</em>) type transition on PLite&#8217;s parallel value iteration solver. First, we wrap our existing code under the module <cite>ExampleModule</cite> (you can name it whatever you want), and save it under the file name <cite>ExampleModule.jl</cite>. As our naming scheme suggests, the module and file should share the same name. Below is what should be saved to the file.</p>
<p><a href="#id56"><span class="problematic" id="id57">``</span></a><a href="#id58"><span class="problematic" id="id59">`</span></a>julia
module ExampleModule</p>
<dl class="docutils">
<dt>export</dt>
<dd>mdp,
solver,
solve,
getpolicy</dd>
</dl>
<p>using PLite</p>
<p># constants
const MinX = 0
const MaxX = 100
const StepX = 20</p>
<p># mdp definition
mdp = MDP()</p>
<p>statevariable!(mdp, &#8220;x&#8221;, MinX, MaxX)  # continuous
statevariable!(mdp, &#8220;goal&#8221;, [&#8220;no&#8221;, &#8220;yes&#8221;])  # discrete</p>
<p>actionvariable!(mdp, &#8220;move&#8221;, [&#8220;W&#8221;, &#8220;E&#8221;, &#8220;stop&#8221;])  # discrete</p>
<dl class="docutils">
<dt>transition!(mdp,</dt>
<dd><p class="first">[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;],
function mytransition(x::Float64, goal::AbstractString, move::AbstractString)</p>
<blockquote>
<div><dl class="docutils">
<dt>function isgoal(x::Float64)</dt>
<dd><dl class="first docutils">
<dt>if abs(x - MaxX / 2) &lt; StepX</dt>
<dd>return &#8220;yes&#8221;</dd>
<dt>else</dt>
<dd>return &#8220;no&#8221;</dd>
</dl>
<p class="last">end</p>
</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if isgoal(x) == &#8220;yes&#8221; &amp;&amp; goal == &#8220;yes&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p>end</p>
<dl class="docutils">
<dt>if move == &#8220;E&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.9),
([x - StepX, isgoal(x - StepX)], 0.1)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.2),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.1),
([x + StepX, isgoal(x + StepX)], 0.8)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;W&#8221;</dt>
<dd><dl class="first docutils">
<dt>if x &gt;= MaxX</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.9)]</dd>
</dl>
</dd>
<dt>elseif x &lt;= MinX</dt>
<dd>return [
([x, isgoal(x)], 0.9),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
<dt>else</dt>
<dd><dl class="first last docutils">
<dt>return [</dt>
<dd>([x, isgoal(x)], 0.1),
([x - StepX, isgoal(x - StepX)], 0.8),
([x + StepX, isgoal(x + StepX)], 0.1)]</dd>
</dl>
</dd>
</dl>
<p class="last">end</p>
</dd>
<dt>elseif move == &#8220;stop&#8221;</dt>
<dd>return [([x, isgoal(x)], 1.0)]</dd>
</dl>
<p>end</p>
</div></blockquote>
<p class="last">end</p>
</dd>
</dl>
<p>)</p>
<dl class="docutils">
<dt>reward!(mdp,</dt>
<dd><p class="first">[&#8220;x&#8221;, &#8220;goal&#8221;, &#8220;move&#8221;],
function myreward(x::Float64, goal::String, move::String)</p>
<blockquote>
<div><dl class="docutils">
<dt>if goal == &#8220;yes&#8221; &amp;&amp; move == &#8220;stop&#8221;</dt>
<dd>return 1</dd>
<dt>else</dt>
<dd>return 0</dd>
</dl>
<p>end</p>
</div></blockquote>
<p class="last">end</p>
</dd>
</dl>
<p>)</p>
<p># solver options
solver = ParallelValueIteration()
discretize_statevariable!(solver, &#8220;x&#8221;, StepX)</p>
</div>
<div class="section" id="end">
<h1>end<a class="headerlink" href="#end" title="Permalink to this headline">¶</a></h1>
<p>On top of the keyword arguments available to <cite>SerialValueIteration</cite>, <cite>ParallelValueIteration</cite> has an additional <cite>nthreads</cite> keyword argument. The default value is <cite>CPU_CORES / 2</cite>.</p>
<p><cite>CPU_CORES</cite> is a Julia standard library constant, and it defaults to the number of CPU cores in your system. But the number of cores given usually includes virtual cores (e.g., Intel processors), so we divide by two to obtain the number of physical cores. There isn&#8217;t an issue with increasing the number of cores. But since we have the same number of cores doing the same number of work, there won&#8217;t be an increase in efficiency. In fact, with greater number of threads there may be more overhead and runtime processes. As such, we recommend using as many threads as there are physical cores on the machine. In the case of the parallel solver, we can define</p>
<p><a href="#id60"><span class="problematic" id="id61">``</span></a><a href="#id62"><span class="problematic" id="id63">`</span></a>julia
solver = ParallelValueIteration(</p>
<blockquote>
<div>tol=1e-6,
maxiter=10000,
discount=0.999,
verbose=false,
nthreads=10)</div></blockquote>
<p><a href="#id64"><span class="problematic" id="id65">``</span></a><a href="#id66"><span class="problematic" id="id67">`</span></a></p>
<p>As in the serial solver, PLite needs a definition of the discretization scheme.</p>
<p>Notice that there are two modifications to the code being wrapped (in addition to putting it in <cite>ExampleModule</cite> and using <cite>ParallelValueIteration</cite>):</p>
<ol class="arabic simple">
<li>we removed the <cite>solve</cite> bit that generated the solution</li>
<li>we added the <cite>export</cite> keyword that makes the objects and functions available to the user (either in on the console or the Jupyter notebook)</li>
</ol>
<p><a href="#id68"><span class="problematic" id="id69">``</span></a><a href="#id70"><span class="problematic" id="id71">`</span></a>julia
export</p>
<blockquote>
<div>mdp,
solver,
solve,
getpolicy</div></blockquote>
<p><a href="#id72"><span class="problematic" id="id73">``</span></a><a href="#id74"><span class="problematic" id="id75">`</span></a></p>
<p>On the console or Jupyter notebook, we then input the following.</p>
<p><a href="#id76"><span class="problematic" id="id77">``</span></a><a href="#id78"><span class="problematic" id="id79">`</span></a>julia
const NThreads = int(CPU_CORES / 2)
addprocs(NThreads - 1)  # -1 to account for existing process</p>
<p>using ExampleModule</p>
<p># generate results
solution = solve(mdp, solver)
<a href="#id80"><span class="problematic" id="id81">``</span></a><a href="#id82"><span class="problematic" id="id83">`</span></a></p>
<p>Notice we add the desired number of processes before loading the module. This sequence of code evaluation allows all processes to get the code on ExampleModule&lt;sup&gt;[4](#myfootnote4)&lt;/sup&gt;. We then call <cite>solve</cite> on the mdp and solver to obtain the solution.</p>
<p>### Monte-Carlo Tree Search</p>
<p>The Monte-Carlo tree search (MCTS) algorithm relies on the same problem definition framework as the value iteration algorithms. Like value iteration, MCTS works by keeping an internal approximation of <em>Q*(*s</em>,*a*)* values and chooses the action that maximizes this state-action utility. Unlike value iteration, however, MCTS is an online algorithm. This means that the MCTS policy may start off poor, but it gets better the more it interacts with the MDP simulator/environment.</p>
<p>Note that a key assumption is that both the action space and the state space are finite&lt;sup&gt;[4](#myfootnote5)&lt;/sup&gt;. Otherwise, we will keep selecting unexplored actions, and no node of depth higher than one would be added. Thus, after the state and action spaces are discretized, the algorithm only works with these discrete states and actions. For the transition function in our implementation, we will map the given current state and action pair to the closest discrete state and action. Additionally, the next state transitioned into (via sampling) will also be mapped to the closest discrete state.</p>
<p>The main advantage to MCTS is its ability to give a good approximation of the state-action utility function despite not needing an expensive value iteration-type computation. We recommend using this for problems with large state and/or action spaces.</p>
<p>The syntax for using a serial MCTS solver is similar to that of the serial value iteration solver. We still need to discretize continuous variables since our solver implements the finite MCTS. Otherwise, the only difference is having to initialize a different type of solver.</p>
<p><a href="#id84"><span class="problematic" id="id85">``</span></a><a href="#id86"><span class="problematic" id="id87">`</span></a>julia
# solver options
solver = SerialMCTS()
discretize_statevariable!(solver, &#8220;x&#8221;, StepX)</p>
<p># generate results
solution = solve(mdp, solver)
<a href="#id88"><span class="problematic" id="id89">``</span></a><a href="#id90"><span class="problematic" id="id91">`</span></a></p>
<p>There are four keyword arguments we can use to instantiate the solver: <cite>niter</cite>, <cite>maxdepth</cite>, <cite>exex</cite>, and <cite>discount</cite>. These parameters correspond to the number of iterations during each action selection when querying the MCTS policy object (see more in the [Solution](#solution) section), the maximum depth of the search tree used in MCTS, a constant that varies the exploration-exploitation preference, and the simulation/rollout discount factor, respectively.</p>
<p>The default parameters are</p>
<ul class="simple">
<li><cite>niter = 50</cite></li>
<li><cite>maxdepth = 20</cite></li>
<li><cite>exex = 3.0</cite></li>
<li><cite>discount = 0.99</cite>.</li>
</ul>
<p>## Solution</p>
<p>The solution to the MDP generated by any solver can be extracted in the form of a policy function. The policy function takes as arguments the state variables in the same order defined for the transition and reward functions.</p>
<p>To obtain the policy from the <cite>solution</cite> to the <cite>mdp</cite> defined above, we call</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">policy</span> <span class="pre">=</span> <span class="pre">getpolicy(mdp,</span> <span class="pre">solution)</span>
<span class="pre">`</span></code></p>
<p>If we want to query the optimal policy to take at the state <cite>stateq = (12, &#8220;no&#8221;)</cite>, we can pass the query to the policy function as follows.</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">actionq</span> <span class="pre">=</span> <span class="pre">policy(stateq...)</span>
<span class="pre">`</span></code></p>
<p>Above, <cite>actionq</cite> takes on the value <cite>&#8220;E&#8221;</cite>. This action makes sense since we&#8217;re to the west of the midpoint goal for the problem, and moving east would bring us closer to the goal. Note that we used the trailing ellipsis <cite>...</cite> to expand <cite>stateq</cite>. An equally valid function call is</p>
<p><code class="docutils literal"><span class="pre">`julia</span>
<span class="pre">actionq</span> <span class="pre">=</span> <span class="pre">policy(12,</span> <span class="pre">&quot;no&quot;)</span>
<span class="pre">`</span></code></p>
<p>For online policies, like those generated by MCTS, the policy generally improves as it receives more queries.</p>
<p># Todos</p>
<p>## Short-term
* [ ] add Jupyter notebook examples
* [ ] include generic unit tests for solvers
* [ ] add support for pomdps (qmdp and fib)</p>
<p>## Medium-term
* [ ] link with pomdps.jl</p>
<p># Footnotes</p>
<p>&lt;a name=&#8221;myfootnote1&#8221;&gt;1&lt;/a&gt;:
Because that&#8217;s what Hao Yi, the author, aspires to be. :neutral_face:</p>
<p>&lt;a name=&#8221;myfootnote2&#8221;&gt;2&lt;/a&gt;:
If we want to run a program in parallel, we need our code to be available on all processes that run it. At startup, the default number of processes allocated to Julia is one. We can check this by evaluating <cite>nprocs()</cite>. To increase the number of processes by <cite>moreprocs</cite>, we call <cite>addprocs(moreprocs)</cite>.</p>
<p>When you input code to, say, the Julia console or Jupyter notebook, it&#8217;s only available to the main process, whichever one executes the code. This is true even if we called <cite>addprocs</cite> before the code is input. One way to load the code to all processes is to use the <cite>using</cite> keyword on a module. We adopt this approach.</p>
<p>&lt;a name=&#8221;myfootnote3&#8221;&gt;3&lt;/a&gt;:
At the very least, what the author thinks the simplest way to use parallel value iteration. File an issue on Github if you can think of a better way to do this without exposing the user to complicated stuff under the hood. :persevere:</p>
<p>&lt;a name=&#8221;myfootnote4&#8221;&gt;4&lt;/a&gt;:
Although the objects and functions exported by <cite>ExampleModule</cite> is only available to the main process that runs on the console or Jupyter notebook.</p>
<p>&lt;a name=&#8221;myfootnote5&#8221;&gt;5&lt;/a&gt;:
There are ways to deal with the problem of infinite support of the probability distribution underlying the transition, and infinite action spaces, for our version of MCTS. For more information, consider Adrien Couetoux&#8217;s &#8220;Monte Carlo Tree Search for Continuous and Stochastic Sequential Decision Making Problems.&#8221;</p>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">)</a></li>
<li><a class="reference internal" href="#id26">)</a></li>
<li><a class="reference internal" href="#id31">)</a></li>
<li><a class="reference internal" href="#end">end</a></li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2015, Hao Yi Ong.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.3.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.3</a>
      
      |
      <a href="_sources/index.txt"
          rel="nofollow">Page source</a></li>
    </div>

    

    
  </body>
</html>