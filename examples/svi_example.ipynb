{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Serial Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first load PLite into the current process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "push!(LOAD_PATH, \"../src\")\n",
    "using PLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "For this example, we define a simple 1-D gridworld type problem where the goal of the agent is to move and stop at the center of the grid. \n",
    "\n",
    "The idea of this section is to define the problem mathematically without fussing about how we might choose to solve it later (e.g., use a discretized representation). In general, given the wide variety of MDP solvers available, it's important not to restrict our problem representation by what solver we might eventually choose. Rather, we want to choose the best solver after understanding our problem (e.g., find some problem structure that we can exploit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MDP initialization\n",
    "\n",
    "We first define some constants and initialize the empty `MDP` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLite.MDP(Dict{AbstractString,PLite.LazyVar}(),Dict{AbstractString,PLite.LazyVar}(),PLite.LazyFunc(true,ASCIIString[],##emptyfunc#7460),PLite.LazyFunc(true,ASCIIString[],##emptyfunc#7459))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constants\n",
    "const MinX = 0\n",
    "const MaxX = 100\n",
    "const StepX = 20\n",
    "\n",
    "# mdp definition\n",
    "mdp = MDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State and action spaces definition\n",
    "\n",
    "We define the state and action spaces using a factored representation. If we can't factor the representation, we can simply define the space using a single discrete or continuous variable. \n",
    "\n",
    "For each state and action variable, we either define a continuous or discrete variable. Note that we use the state variables' \"natural representation\" and avoid discretizing `x` even though for value iteration we would eventually have to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLite.ValuesVar(\"move\",ASCIIString[\"W\",\"E\",\"stop\"])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state space\n",
    "statevariable!(mdp, \"x\", MinX, MaxX)  # continuous\n",
    "statevariable!(mdp, \"goal\", [\"no\", \"yes\"])  # discrete\n",
    "\n",
    "# action space\n",
    "actionvariable!(mdp, \"move\", [\"W\", \"E\", \"stop\"])  # discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition and reward functions definition\n",
    "\n",
    "To define either the transition or reward function, we pass in the `MDP` object, the transition function itself, and an ordered set of the transition function's argument names. \n",
    "\n",
    "In the case of the transition function, we can define it using either the $T(s,a)$ or $T(s,a,s')$ format. Our example here uses the former way. In the case of the reward function, we define it using the $R(s,a)$ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Base.String is deprecated, use AbstractString instead.\n",
      "  likely near In[5]:1\n",
      "WARNING: Base.String is deprecated, use AbstractString instead.\n",
      "  likely near In[5]:1\n",
      "WARNING: Base.String is deprecated, use AbstractString instead.\n",
      "  likely near In[5]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PLite.LazyFunc(false,ASCIIString[\"x\",\"goal\",\"move\",\"x\",\"goal\"],mytransition)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition!(mdp,\n",
    "  [\"x\", \"goal\", \"move\", \"x\", \"goal\"],  # note |xp| is an \"x\" variable\n",
    "                                       # note (s,a,s') order\n",
    "  function mytransition(\n",
    "      x::Float64,\n",
    "      goal::AbstractString,\n",
    "      move::AbstractString,\n",
    "      xp::Float64,\n",
    "      goalp::AbstractString)\n",
    "\n",
    "    function internaltransition(x::Float64, goal::AbstractString, move::AbstractString)\n",
    "      function isgoal(x::Float64)\n",
    "        if abs(x - MaxX / 2) < StepX\n",
    "          return \"yes\"\n",
    "        else\n",
    "          return \"no\"\n",
    "        end\n",
    "      end\n",
    "\n",
    "      if isgoal(x) == \"yes\" && goal == \"yes\"\n",
    "        return [([x, isgoal(x)], 1.0)]\n",
    "      end\n",
    "\n",
    "      if move == \"E\"\n",
    "        if x >= MaxX\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.9),\n",
    "            ([x - StepX, isgoal(x - StepX)], 0.1)]\n",
    "        elseif x <= MinX\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.2),\n",
    "            ([x + StepX, isgoal(x + StepX)], 0.8)]\n",
    "        else\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.1),\n",
    "            ([x - StepX, isgoal(x - StepX)], 0.1),\n",
    "            ([x + StepX, isgoal(x + StepX)], 0.8)]\n",
    "        end\n",
    "      elseif move == \"W\"\n",
    "        if x >= MaxX\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.1),\n",
    "            ([x - StepX, isgoal(x - StepX)], 0.9)]\n",
    "        elseif x <= MinX\n",
    "          return [\n",
    "          ([x, isgoal(x)], 0.9),\n",
    "          ([x + StepX, isgoal(x + StepX)], 0.1)]\n",
    "        else\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.1),\n",
    "            ([x - StepX, isgoal(x - StepX)], 0.8),\n",
    "            ([x + StepX, isgoal(x + StepX)], 0.1)]\n",
    "        end\n",
    "      elseif move == \"stop\"\n",
    "        return [([x, isgoal(x)], 1.0)]\n",
    "      end\n",
    "    end\n",
    "\n",
    "    statepprobs = internaltransition(x, goal, move)\n",
    "    for statepprob in statepprobs\n",
    "      if xp == statepprob[1][1] && goalp == statepprob[1][2]\n",
    "        return statepprob[2]\n",
    "      end\n",
    "    end\n",
    "    return 0\n",
    "\n",
    "  end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Infinite horizon MDP\n",
    "\n",
    "For an infinite horizon problem with discount $\\gamma$, it can be proven that the value of an optimal policy satisfies the *Bellman equation*\n",
    "$$U^{\\star}(s) = \\max_{a}\\left( R\\left(s,a\\right) + \\gamma\\sum_{s'}T\\left(s'\\mid s, a\\right) U^{\\star}\\left(s'\\right) \\right)\\text{.}$$\n",
    "For the convergence proof, see http://web.stanford.edu/class/ee266/lectures/dpproof.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finite horizon MDP\n",
    "\n",
    "In value iteration, we compute optimal value function $U_{n}$ associated with a finite horizon of $n$ and no discounting. If $n=0$, then $U_{0}(s)=0$ for all $s$. We can compute $U_{n}$ recursively from this base case\n",
    "$$U_{n}(s) = \\max_{a}\\left( R\\left(s,a\\right) + \\sum_{s'}T\\left(s'\\mid s, a\\right) U_{n-1}\\left(s'\\right) \\right)\\text{.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policy extraction\n",
    "\n",
    "The optimal value function $U^{\\star}$ appears on both sides of the equation. Value iteration approximates $U^{\\star}$ by iteratively updating the estimate of $U^{\\star}$ using the above equation. Once we know $U^{\\star}$, we can extract an optimal policy via\n",
    "$$\\pi\\left(s\\right) \\leftarrow \\underset{a} {\\mathrm{argmax}} \\left( R\\left(s,a\\right) + \\gamma\\sum_{s'}T\\left(s'\\mid s, a\\right) U^{\\star}\\left(s'\\right) \\right)\\text{.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
